{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "import torch\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GATConv, GraphConv\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import dgl.nn as dglnn\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentence1 = \"I own a cat.\"\n",
    "sentence2 = \"he owns a cat.\"\n",
    "\n",
    "embedding1 = model.encode(sentence1)\n",
    "embedding2 = model.encode(sentence2)\n",
    "\n",
    "np.shape(embedding1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "similarity = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "print(f\"Cosine Similarity: {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G = nx.DiGraph()\n",
    "file='training/ES2002a.txt'\n",
    "# Read the file and add edges\n",
    "with open(file, 'r') as file:\n",
    "    for line in file:\n",
    "        source, relation, target = line.strip().split()\n",
    "        G.add_edge(source, target, label=relation)\n",
    "plt.figure(figsize=(50, 50))\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)  # positions for all nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700)\n",
    "nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20)\n",
    "nx.draw_networkx_labels(G, pos, font_size=12, font_family=\"sans-serif\")\n",
    "\n",
    "# Draw edge labels\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_from_key(file_path, key):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Access the value using the key\n",
    "    # Note: This assumes the key is at the top level of the JSON structure\n",
    "    return data.get(key, \"Key not found\")\n",
    "\n",
    "\n",
    "file_path = 'training_labels.json'\n",
    "key = 'IS1003d'\n",
    "value = get_value_from_key(file_path, key)\n",
    "print(len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id= \"ES2002b\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "g= preprocessing.ProcessDialogue(file_id, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=897, num_edges=896,\n",
       "      ndata_schemes={'features': Scheme(shape=(768,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
       "      edata_schemes={'rel_type': Scheme(shape=(14,), dtype=torch.float32)})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat engineering ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing tokeninzing\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenize and prepare the inputs\n",
    "# text = \"My name is hajar.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)\n",
    "\n",
    "# # Get the embeddings\n",
    "# embeddings = output.last_hidden_state\n",
    "# embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more embeddings than initial tokens in the sentence due to special tokens : [CLS] and [SEP] and if the words aren't in the BERT wordPiece subword tokenization system, it break it down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BertForSequenceClassification \n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(input_ids)\n",
    "#     predictions = torch.argmax(outputs.logits, axis=-1)\n",
    "\n",
    "# df_copy = df.copy()\n",
    "\n",
    "# df_copy['summary_class'] = predictions.numpy()\n",
    "# df_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst = g.edges()\n",
    "\n",
    "# # Create reverse edges\n",
    "# src, dst = torch.cat([src, dst]), torch.cat([dst, src])\n",
    "\n",
    "# # Create a new graph with edges in both directions\n",
    "# g_undirected = dgl.graph((src, dst))\n",
    "# g_undirected.ndata['features'] = g.ndata['features']\n",
    "# labels_tensor = torch.tensor(df['label'].to_numpy(), dtype=torch.long)\n",
    "# g_undirected.ndata['label'] = labels_tensor\n",
    "# if 'rel_type' in g.edata:\n",
    "#     g_undirected.edata['rel_type'] = torch.cat([g.edata['rel_type'], g.edata['rel_type']])\n",
    "\n",
    "\n",
    "\n",
    "# g=g_undirected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
